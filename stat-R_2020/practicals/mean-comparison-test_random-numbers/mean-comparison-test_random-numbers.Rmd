---
title: "Tutorial -- Comparing means under controlled conditions"
author: "Jacques van Helden"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    highlight: zenburn
    self_contained: no
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  beamer_presentation:
    colortheme: dolphin
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    incremental: no
    keep_tex: no
    slide_level: 2
    theme: Montpellier
    toc: yes
  ioslides_presentation:
    colortheme: dolphin
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    smaller: yes
    toc: yes
    widescreen: yes
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    fig_caption: yes
    highlight: zenburn
    toc: yes
    toc_depth: 3
  slidy_presentation:
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    highlight: tango
    incremental: no
    keep_md: yes
    smaller: yes
    theme: cerulean
    toc: yes
    widescreen: yes
font-import: http://fonts.googleapis.com/css?family=Risque
subtitle: Diplôme Universitaire en Bioinformatique Intégrative (DUBii)
font-family: Garamond
transition: linear
---

```{r include=FALSE, echo=FALSE, eval=TRUE}
library(knitr)

options(width = 300)
knitr::opts_chunk$set(
  fig.width = 7, fig.height = 5, 
  fig.path = 'figures/mean-compa-tests_',
  fig.align = "center", 
  size = "tiny", 
  echo = TRUE, eval = TRUE, 
  warning = FALSE, message = FALSE, 
  results = TRUE, comment = "")
# knitr::asis_output("\\footnotesize")

```


## Required libraries

Load the following libraries or install them if required. 

```{r}
require(knitr)
```


## Introduction

This tutorial aims at providing an empirical introduction to the application of mean comparison tests to omics data.

The goals include

- revisiting the **basic underlying concepts** (sampling, estimation, hypothesis testing, risks...);
- perceiving the problems that arise when a test of hypothesis is applied on several thousand of features (**multiple testing**);
- introducing some methods to circumvent these problems (**multiple testing corrections**);
- using graphical representations in order to grasp the results of several thousand tests in a winkle of an eye: 

    - p-value histogram
    - MA plot
    - volcano plot
  

The whole tutorial will rely on artificial data generated by drawing random numbers that follow a given distribution  of probabilities (in this case, the normal distribution, but other choices could be made afterwards).


The tutorial will proceed progressively: 

1. Generate a multivariate table (with *individuals* in columns and *features* in rows) and fill it with random data following a given distribution of probability.

2. Measure different descriptive parameters on the sampled data.

3. Use different graphical representations to visualise the data distribution. 

4. Run a test of hypothesis on a given feature.

5. Run the same test of hypothesis on all the features. 

6. Use different graphical representations to summarize the results of all the tests.

7. Apply different corrections for multiple testing (Bonferroni, Benjamini-Hochberg, Storey-Tibshirani q-value). 

8. Compare the performances of the test depending on the chosen multiple testing correction. 

## Experimental setting

Well, by "experimental" we mean here that we will perform *in silico* experiments. 

Let us define the parameters of our analysis. We will generate data tables of artificial data following normal distributions, with either different means (tests under $H_1$) or equal means (tests under $H_0$). 

We will do this for a number of features  $m_0=10,000$ (number of rows in the "$H_0$" data table), which could be considered as replicates to study the impact of sampling fluctuations. 

In a second time (not seen here) we could refine the script by running a sampling with a different mean for each feature, in order to mimic the behaviour of omics datasets (where genes have different levels of expression, proteins and metabolite different concentrations). 


### Parameters

| Parameter | Value | Description | Value |
|---------|-------------|---------|
| $n_1$  |  ${2, 3, 4, 8, 16, 32, 64}$ | size of the sample from the first population. individual choice. Each participant will choose a given sample size |
| $n_2$  |  $= n_1$ | size of the sample from the second population |
| $\mu_1$ | 10 or 7 | mean of the first population. each participant will chose one value |
| $\mu_2$ | 10 | mean of the second population |
| $\sigma_1$ | 2 | Standard deviation of the first population |
| $\sigma_2$ | 3 | Standard deviation of the second population |
| $m_0$$ | $10,000$ | number of features under null hypothesis |




### Sample sizes

Each participant will choose a different sample size among the following values: $n  \in {2, 3, 4, 5, 8, 16, 64}$. Noteowrthy, many omics studies are led with a very small number of replicates (frequently 3), so that it will be relevant to evaluate thee impact of the statistical sample size (number of replicates) on the sensibility (proportion of cases declared positive under $H_1$).  



## Performances of the tests

We will measure the performances of a test by running $r = 10,000$ times under $H_0$, and $r = 10,000$ times under $H_1$. 

- count the number of $FP$, $TP$, $FN$, $TN$
- compute the derived statistics: $FPR$, $FDR$ and $Sn$

$$FPR = \frac{FP}{m_0} = \frac{FP}{FP + TN} $$

$$FDR = \frac{FP}{\text{Positive}} = \frac{FP}{FP + TP} $$

$$Sn = \frac{TP}{m_1} = \frac{TP}{TP + FN} $$
$$PPV = \frac{TP}{\text{Positive}} = \frac{TP}{TP + FP}$$

## Recommendations


### Coding recommendations

1. Choose a consistent coding style, consistent with a reference  style guide (e.g. [Google R Syle Guide](https://google.github.io/styleguide/Rguide.html)). In particular: 

    - For variable names, use the camel back notation with a leading lowercase (e.g. `myDataTable`) rather than the dot separator (`my.data.table`)
    - For variable names, use the camel back notation with a leading uppercase (e.g. `MyMeanCompaTest`). 

2. Define your variables with explicit names (sigma, mu rather than a, b, c, ...).

3. Comment your code

    - indicate what each variable represents
    - before each segment of code, explain what it will do

4. Ensure consistency between the code and the report $\rightarrow$ inject the actual values of the R variables in the markdown. 

### Scientific recommendations

1. Explicitly formulate the statistical hypotheses before running a test.

2. Discuss the assumptions underlying the test: are they all fulfilled? If not explain why (e.g. because we want to test the impact of this parameter, ...) .



## Tutorial



### Part 1: generating random datasets

#### Define your parameters

Write a block of code to define the parameters specified aboce. 

Note that each participant will have a different value for the sample sizes ($n_1, n_2$). 

```{r parameters}
#### Defining the parameters ####

## Sample sizes. 
## This parameter should be defined individually for each participant
n1 <- 8 # sample size for the first group
n2 <- 8 # ssample size for second group

## First data table
m <- 10000 # Number of features
mu1 <- 7 # mean of the first population
mu2 <- 10 # mean of the second population

## Standard deviations
sigma1 <- 2 # standard deviation of the first population
sigma2 <- 3 # standard deviation of the second population

## Significance threshold. 
## Note: will be applied successively on the different p-values
## (nominal, corrected) to evaluate the impact
alpha <- 0.05

```

The table below lists the actual values for my parameters (your values for $n_i$ will be different). 

| Parameter | Value | Description | 
|-----------|-------|-------------------|
| $\mu_1$ | `r mu1` | Mean of the first population | 
| $\mu_2$ | `r mu2` | Mean of the second population | 
| $\sigma_1$ | `r sigma1` | Standard deviation of the first population | 
| $\sigma_2$ | `r sigma1` | Standard deviation of the second population | 
| $n_1$ | `r n1` | Sample size for the first group | 
| $n_2$ | `r n2` | Sample size for the second group | 


#### Generate random data sets

Exercise: 

- Generate an data frame named `group1` which with $m_0$ rows (the number of features under $H_0$, defined above) and $n_1$ columns (sample size for the first population), filled with random numbers drawn from the first population. 

- Name the columns with labels indicating the group and sample number: `grp1s1`, , `grp1s2` ... with indices from 1 $n_1$.

- Name the rows to denote the feature numbers: `h0ft1`, `h0ft2`, ... with indices from 1 to $m_0$. 

- Check the dimensions of `group1`.

- Print its first and last rows to check its content and the row and column names.

- Generate a second data frame named `group2` for the samples drawn from the second population with the appropriate size, and name the columns and rows consistently. 


**Useful functions**

- `rnorm()`
- `matrix()`
- `data.frame()`
- `paste()`
- `paste0()`
- `colnames()`
- `rownames()`
- `dim()`

**Trick**: 

- the funciton `matrix()` enables us to define the number of columns and the number of rows, 
- the function `data.frames()` does not enable this, but it can take as input a matrix, from which it will inherit the dimensions

**Solution** (click on the "code" button to check the solution). 

```{r}
#### Generating a random data frame of the appropriate size ####

## Dataset under H0
group1 <- data.frame(
  matrix(data = rnorm(n = m * n1, mean = mu1, sd = sigma1),
         nrow = m, 
         ncol = n1))
colnames(group1) <- paste(sep = "", "grp1s", 1:n1)
rownames(group1) <- paste(sep = "", "h0ft", 1:m)

## Dataset under h1
group2 <- data.frame(
  matrix(data = rnorm(n = m * n2, mean = mu2, sd = sigma2),
         nrow = m, 
         ncol = n2))
colnames(group2) <- paste(sep = "", "grp2s", 1:n2)
rownames(group2) <- paste(sep = "", "h0ft", 1:m)

```

Check the content of the data table from the first group. 

```{r}
dim(group1)
kable(head(group1))
kable(tail(group1))
```

Check the content of the data table from the second group. 

```{r}
dim(group2)
kable(head(group2))
kable(tail(group2))
```

### Checking the properties of the result table

Check the properties of the columns (individuals, e.g. biological samples) and rows (features, e.g. genes or proteins or metabolites) of the data tables. 

- Use the `summary()` funciton to quickly inspect the column-wise properties (statistics per individual).

- Use `apply()`,  `mean()` and `sd()` to generate a data frame that collects

    - the column-wise parameters (statistics per feature)
    - the row-wise  parameters (statistics per feature).  

```{r}
## Columns-wise statistics
colStatsH0 <- data.frame(
    m1 = apply(group1, MARGIN = 2, mean),
    m2 = apply(group2, MARGIN = 2, mean),
    s1 = apply(group1, MARGIN = 2, sd),
    s2 = apply(group2, MARGIN = 2, sd)
  )

## Row-wise statistics
rowStatsH0 <- data.frame(
    m1 = apply(group1, MARGIN = 1, mean),
    m2 = apply(group2, MARGIN = 1, mean),
    s1 = apply(group1, MARGIN = 1, sd),
    s2 = apply(group2, MARGIN = 1, sd)
  )

```

#### Add a column with the difference between sample means for each feature.


**Tips: ** this can be done in a single operation. 


```{r}
rowStatsH0$diff <- rowStatsH0$m2 - rowStatsH0$m1
```

#### Visualisation of the data

- Draw two histograms with all the values group 1 and group2, respectively. 

**Tip: ** use `mfrow()` to display the histogram above each other, and set the limits of the abscissa (x axis) to the same value. 

- Draw histogram with the sampling distribution of the means in the respective groups. 

- Compare the standard deviations measures in the sampled values, and in the feature means. Do they differ ? Explain why. 



## Part 2: hypothesis testing

### Run Student test on a given feature

Since we are interested by differences in either directions, we run a two-tailed test. 

Hypotheses: 

$$H_0: \mu_1 = \mu2$$

$$H_1: \mu_1 \ne \mu2$$
**Exercise:** pick up a given feature (e.g. the $267^{th}$) and run a mean comparison test. Choose the parameters according to your experimental setting. 

**Tips:**

- `t.test()`
- you need to choose the test depending on whether the two populations have equal variance (Student) or not (Welch). Since we defined different values for the populations standard deviations ($\sigma_1$, $\sigma_2$), the choise is obvious. 

```{r one_t_test}
i <- 267 ## Pick up a given feature, arbitrarily

## Select the values for this feature in the group 1 and group 2, resp. 
## Tip: I use unlist() to convert a single-row data.frame into a vector
x1 <- unlist(group1[i, ]) 
x2 <- unlist(group2[i, ])

## Run Sudent t test on one pair of samples
t.result <- t.test(
  x = x1, y = x2, 
  alternative = "two.sided", var.equal = FALSE)

## Print the result of the t test
print(t.result)

## Compute some additional statistics about the samples
mean1 <- mean(x1) ## Mean of sample 1
mean2 <- mean(x2) ## Mean of sample 2
d <- mean2 - mean1 ## Difference between sample means

```



###  Interpret the result

The difference between sample means was $d = `r round(digits = 2, d)`$. 


The $t$ test computed the $t$ statistics, which standardizes this observed distance between sample means relative to the estimated variance of the population, and to the sample sizes. With the random numbers generated above, the value is $t_{obs} = `r round(digits = 4, t.result$statistic)`$. 

The corresponding p-value is computed as the sum of the area of the left and right tails of the Student distribution, with $\nu = n_1 + n_2 - 2 = `r t.result$parameter`$ degrees of freedom. It  indicates the probability of obtaining by chance -- ***under the null hypothesis*** -- a result at least as extreme as the one we observed.


In our case, we obtain $p = P(T > |t_{obs}| = P(T > `r round(digits=4, abs(t.result$statistic))`) = `r signif(digits = 3, t.result$p.value)`$. This is higher than our threshold $alpha = `r alpha`$. We thus accept the null hypothesis. 



### Replicating the test for each feature

In R, loops are quite inefficient, and it is generally recommended to directly run the computations on whole vectors (R has been designed to be efficient for this), or to use specific functions in order to apply a given function each row / column of a table, or to each element of a list. 

For the sake of simplicity, we will first show how to implement a simple but inefficient code with a loop. In the advanced course (STATS2) will see how to optimize the speed with the `apply()` function. 


```{r t_test_loop}

## Define the statistics we want to collect
result.columns <- c("i", "m1", "m2", "diff", "statistic", "p.value")

## Instantiate a result table to store the results
result.table <- data.frame(matrix(nrow = m, ncol = length(result.columns)))
colnames(result.table) <- result.columns # set the column names
# View(result.table) ## Check the table: it contians NA values
  
## Iterate random number sampling followed by t-tests
for (i in 1:m) {
  ## Generate two vectors containing the values for sample 1 and sample 2, resp.
  x1 <- unlist(group1[i, ]) ## sample 1 values
  x2 <- unlist(group2[i, ]) ## sample 2 values

  ## Run the t test
  t.result <- t.test(
    x = x1, y = x2, 
    alternative = "two.sided", var.equal = FALSE)
  # names(t.result)
  
  ## Collect the selected statistics in the result table
  result.table[i, "i"] <- i
  result.table[i, "statistic"] <- t.result["statistic"]
  result.table[i, "p.value"] <- t.result["p.value"]
  
  ## Compute some additional statistics about the samples
  result.table[i, "m1"] <- mean(x1) ## Mean of sample 1
  result.table[i, "m2"] <- mean(x2) ## Mean of sample 2
  result.table[i, "diff"] <- mean(x2) - mean(x1)  ## Difference between sample means

}

## View(result.table)

```

### Distribution of the observed differences for the $`r m`$ iterations of the test

```{r diff_histo, fig.width=6, fig.height=8, out.width="50%"}
par(mfrow = c(2, 1))
#head(result.table)
## Draw an histogram of the observed differences
max.diff <- max(abs(result.table$diff))
hist(result.table$diff, breaks = 100, 
     col = "#BBFFDD", las = 1, 
     xlim = c(-max.diff, max.diff), ## Make sure that the graph is centered on 0
     main = "Differences between means",
     xlab = "Difference between means",
     ylab = "Number of tests")
abline(v = 0)

max.t <- max(abs(result.table$statistic))
hist(result.table$statistic, breaks = 100, 
     col = "#BBFFDD", las = 1,
     xlim = c(-max.t, max.t), ## Make sure that the graph is centered on 0
     main = "T statistics",
     xlab = "T-test statistic",
     ylab = "Number of tests")
par(mfrow = c(1, 1))
abline(v = 0)

```


### P-value histogram

```{r pval_histo, fig.width=7, fig.height=5, out.width="50%"}
## Draw an histogram of p-values with 20 bins
hist(result.table$p.value, breaks = 20,
     col = "#BBFFDD", las = 1,
     main = "P-value histogram",
     xlab = "T-test P-value",
     ylab = "Number of tests")

## Draw a horizontal line indicating the number of tests per bin that would be expected under null hypothesis
abline(h = m / 20, col = "darkgreen", lwd = 2)
```

## Creating a function to reuse the same code with different parameters

Depending on the selected task in the assignments above, we will run different tests with different parameters and compare the results. The most rudimentary way to do this is top copy-paste the chunk of code above for each test and set of parameters required for the assigned tasks. 

However, having several copies of an almost identical block of code is a very bad pracrice in programming, for several reasons

- lack of readability: the code rapidly becomes very heavy;
- difficulty to maintain: any modification has to be done on each copy of the chunk of code;
- risk for consistency: this is a source of inconsistency, because at some moment we will modify one copy and forget another one.

A better practice is to define a **function** that encapsulates the code, and enables to modify the parameters by passing them as **arguments*. Hereafter we define a function that 

- takes the parameters of the analysis as arguments

    - population means $\mu_1$ and $\mu_2$, 
    - population standard deviations $\sigma_1$ and $\sigma_2$, 
    - sample sizes $n_1$ and $n_2$,
    - number of iterations $r$.
    
- runs $r$ iterations of the t-test with 2 random samples, 
- returns the results in a table with one row per iteration, and one column per resulting statistics (observed $t$ score, p-value, difference between means, ...);


```{r iterate_t_test_function}
## Define a function that runs r iterations of the t-test.
iterate.t.test <- function(mu1 = -0.5, ## Mean of the first population
                           mu2 = 0.5,  ## Mean of the second population
                           sigma1 = 1, ## Standard deviation of the first population
                           sigma2 = 1,  ## Standard deviation of the second population
                           n1 = 10,    ## First sample size
                           n2 = 10,    ## Second sample size
                           r = 10000  ## Iterations
                           ) {
  
  ## Define the statistics we want to collect
  result.columns <- c("i", "m1", "m2", "diff", "statistic", "p.value")
  
  ## Instantiate a result table to store the results
  result.table <- data.frame(matrix(nrow = r, ncol = length(result.columns)))
  colnames(result.table) <- result.columns # set the column names
  # View(result.table) ## Check the table: it contians NA values
  
  ## Iterate random number sampling followed by t-tests
  for (i in 1:r) {
    ## Generate two vectors containing the values for sample 1 and sample 2, resp.
    x1 <- rnorm(n = n1, mean = mu1, sd = sigma1) ## sample 1 values
    x2 <- rnorm(n = n2, mean = mu2, sd = sigma2) ## sample 2 values
    
    ## Run the t test
    t.result <- t.test(
      x = x1, y = x2, 
      alternative = "two.sided", var.equal = TRUE)
    # names(t.result)
    
    ## Collect the selected statistics in the result table
    result.table[i, "i"] <- i
    result.table[i, "statistic"] <- t.result["statistic"]
    result.table[i, "p.value"] <- t.result["p.value"]
    
    ## Compute some additional statistics about the samples
    result.table[i, "m1"] <- mean(x1) ## Mean of sample 1
    result.table[i, "m2"] <- mean(x2) ## Mean of sample 2
    result.table[i, "diff"] <- mean(x2) - mean(x1) ## Difference between sample means
  }
  
  return(result.table) ## This function returns the result table
}

```

This function can then be used several times, with different values of the parameters. 

```{r itrerate_t_test_runs}

## What happens when  the two means are equal (under the null hypothesis)
d0 <- iterate.t.test(mu1 = 0, mu2 = 0)  ## Iterations

## Test increasing values of the difference between means
d0.1 <- iterate.t.test(mu1 = -0.05, mu2 = 0.05)  ## Iterations
d0.2 <- iterate.t.test(mu1 = -0.1, mu2 = 0.1)  ## Iterations
d0.5 <- iterate.t.test(mu1 = -0.25, mu2 = 0.25)  ## Iterations
d1 <- iterate.t.test(mu1 = -0.5, mu2 = 0.5)  ## Iterations
d2 <- iterate.t.test(mu1 = -1, mu2 = 1)  ## Iterations

```


```{r pval_histogram_function}
## Define a function that rdraws the p-value histogram 
## based on the result table of t-test iterations
## as produced by the iterate.t.test() function.
pvalHistogram <- function(
  result.table, ## required input (no default value): the result table from iterate.t.test()
  main = "P-value histogram",  ## main title (with default value)
  alpha = 0.05, ## Significance threshold
  ... ## Additional parameters, which will be passed to hist()
  ) {
  
  ## Plot the histogram
  hist(result.table$p.value, 
       breaks = seq(from = 0, to = 1, by = 0.05),
       las = 1, 
       xlim = c(0,1),
       main = main,
       xlab = "T-test P-value",
       ylab = "Number of tests", ...)
  
  ## Draw a horizontal line indicating the number of tests per bin that would be expected under null hypothesis
  abline(h = m / 20, col = "darkgreen", lwd = 2)
  abline(v = alpha, col = "red", lwd = 2)
  
  ## Compute the percent of positive and negative results``
  nb.pos <- sum(result.table$p.value < alpha)
  nb.neg <- m - nb.pos
  percent.pos <- 100 * nb.pos / m
  percent.neg <- 100 * nb.neg / m
  
  ## Add a legend indicating the percent of iterations declaed positive and negative, resp. 
  legend("topright", bty = "o", bg = "white",
         legend = c(
           paste("% pos =", round(digits = 2, percent.pos)),
           paste("% neg =", round(digits = 2, percent.neg))
         ))
}

```

```{r pval_histograms, fig.width=8, fig.height=8, out.width="90%"}
par(mfrow = c(3, 2)) ## Prepare 2 x 2 panels figure
pvalHistogram(d0, main = "under H0 (mu1 = mu2)", col = "#BBFFDD")
pvalHistogram(d0.1, main = "mu1 - mu2 = 0.1", col = "#FFDDBB")
pvalHistogram(d0.2, main = "mu1 - mu2 = 0.2", col = "#FFBB88")
pvalHistogram(d0.5, main = "mu1 - mu2 = 0.5", col = "#FF8866")
pvalHistogram(d1, main = "mu1 - mu2 = 1", col = "#FF6633")
pvalHistogram(d2, main = "mu1 - mu2 = 2", col = "#FF4422")
par(mfrow = c(1, 1)) ## Restore single-panel layout for next figures

```

## Interpretation of the results

We should now write a report of interpretation, which will address the following questions. 

- Based on the experiments under $H_0$, compute the number of false positives and estimate the **false positive rate** (**FPR**). Compare these values with the **E-value** (expected number of false positives) for the `r m` tests,  and with your $alpha$ trheshold. 

- Based on the experiments under $H_1$, estimate the **sensitivity** (**Sn**) of the test for the different mean differences tested here.

- Interpret the histograms of P-values obtained with the different parameters ?

- Draw a **power curve** (i.e. the sensitivity as a function of the actual difference between population means)

- Discuss about the adequation between the test and the conditions of our simulations. 

- Do these observations correspond to what would be expected? 

The same kind of questions will be asked for the 6 other questions above (impact of sample size,  variance,  non-normality, heteroscadicity, parametric vs non-parametric test). 

