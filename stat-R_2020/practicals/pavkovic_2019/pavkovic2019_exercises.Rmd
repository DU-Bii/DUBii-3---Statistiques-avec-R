---
title: "Practical - exploring omics data"
author: "Olivier Sand and Jacques van Helden"
date: '`r Sys.Date()`'
output:
  html_document:
    self_contained: no
    fig_caption: yes
    highlight: zenburn
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: "hide"
  ioslides_presentation:
    slide_level: 2
    self_contained: no
    colortheme: dolphin
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    smaller: yes
    toc: yes
    widescreen: yes
  slidy_presentation:
    smart: no
    slide_level: 2
    self_contained: yes
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    highlight: tango
    incremental: no
    keep_md: yes
    smaller: yes
    theme: cerulean
    toc: yes
    widescreen: yes
  beamer_presentation:
    colortheme: dolphin
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    incremental: no
    keep_tex: no
    slide_level: 2
    theme: Montpellier
    toc: yes
  revealjs::revealjs_presentation:
    theme: night
    transition: none
    self_contained: true
    css: ../slides.css
  pdf_document:
    fig_caption: yes
    highlight: zenburn
    toc: yes
    toc_depth: 3
  powerpoint_presentation:
    slide_level: 2
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    toc: yes
font-import: http://fonts.googleapis.com/css?family=Risque
subtitle: DUBii 2020
font-family: Garamond
transition: linear
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction


The goal of this practical is to combine different statistical approaches to explore an omics dataset (transcriptome, proteome). 

- graphical representations of the distribution
- impact of the normalisation and standardisation
- multidimensional scaling with Principal Component Analysis (PCA)
- class discovery by clustering


## Normalisation and standardisation

We prepared a notebook (in R markdown) detailing all the dirty cooking details to download the datasets from the Zenodo repository, and to run some preprocessing steps

- log2 transformation
- filtering out of the undetected features (genes, proteins)
- median-based sample-wise centering
- IQR-based sample-wise scaling

## Data loading

We prepared memory images that enable you to reload the preprocessed datasets. 

[R script to reload data](R/01_reload_data.R)

## Instructions

The following tasks will be covered in 2 sessions + a bit of personal work. 


- Write the solution of each exercise in a separate R script file. 
- Take care to properly document the code
- Once the code is satisfying, write a report in R markdown that will incoroporate the R files, and comment the results. 

Alternatively you can immediately write the code inside a R markdown document, as far as it combines a properly documented code and relevant interpretation comments. 


## Exercises



1. Choose one of the datasets and load the memory image


2. Compute sample-wise and feature-wise descriptive statistics 

    - mean, 
    - median, 
    - sd, 
    - var,
    - IQR, 
    - some relevant percentiles (0, 05, 25, 50 , 75, 95, 100)
    
3. Compute the mean value per condition (mean between the replicates)


4. Select the 500 top features according to two different criteria

      a. highest variance
      b. highest log2-ratio between untreated samples and the last day of the experiment

5. Compute different matrices indicating the relations between samples (columns) of the log2-transformed values

    - covariance, 
    - Pearson correlation
    - Spearman correlation 
    - Euclidian distance

Compute these on the commplete dataset and on the 500 matrix selections

6. Use the hclust() method on the seleected features 

    - with different dissimlarity metrics (Euclidian, Pearson, Spearman)
    - with different agglomeration rules (single, average, complete, ward)
    
Plot the feature tree and compare the results obtained with the different choices. 

7. Draw heatmaps of the feature profiles (do not cluster the samples)

8. Run principal component analysis 

    - Use the visualisation tools from Magali Berland's course
    - Plot the sample positions on the 6 first components. Are they consistent with their time and status?






