---
title: "Practical - exploring omics data"
author: "Olivier Sand and Jacques van Helden"
date: '`r Sys.Date()`'
output:
  html_document:
    self_contained: no
    fig_caption: yes
    highlight: zenburn
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: "hide"
  ioslides_presentation:
    slide_level: 2
    self_contained: no
    colortheme: dolphin
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    smaller: yes
    toc: yes
    widescreen: yes
  slidy_presentation:
    smart: no
    slide_level: 2
    self_contained: yes
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    highlight: tango
    incremental: no
    keep_md: yes
    smaller: yes
    theme: cerulean
    toc: yes
    widescreen: yes
  beamer_presentation:
    colortheme: dolphin
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    incremental: no
    keep_tex: no
    slide_level: 2
    theme: Montpellier
    toc: yes
  revealjs::revealjs_presentation:
    theme: night
    transition: none
    self_contained: true
    css: ../slides.css
  pdf_document:
    fig_caption: yes
    highlight: zenburn
    toc: yes
    toc_depth: 3
  powerpoint_presentation:
    slide_level: 2
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    toc: yes
font-import: http://fonts.googleapis.com/css?family=Risque
subtitle: DUBii 2020
font-family: Garamond
transition: linear
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction


The goal of this practical is to combine different statistical approaches to explore an omics dataset (transcriptome, proteome). 

- graphical representations of the distribution
- impact of the normalisation and standardisation
- multidimensional scaling with Principal Component Analysis (PCA)
- class discovery by clustering


## Normalisation and standardisation

We prepared a notebook (in R markdown) detailing all the dirty cooking details to download the datasets from the Zenodo repository, and to run some preprocessing steps

- log2 transformation
- filtering out of the undetected features (genes, proteins)
- median-based sample-wise centering
- IQR-based sample-wise scaling

## Data loading

We prepared memory images that enable you to reload the preprocessed datasets. 

[R script to reload data](R/01_reload_data.R)

## Exercises

1. Choose one of the datasets and load the memory image

2. Explore the dataset 

    - compute the following between-sample matrices: covariance, Pearson's and Spearman's correlation 
    - run principal component analysis and analyse the sample positions on the 6 first components. Are they consistent with their time and status?

3. Select the 500 top features according to two different criteria

      a. highest variance
      b. highest log2-ratio between untreated samples and the last day of the experiment

4. Run clustering on the selected datasets. Study the impact of the (dis)similarity metrics (Euclidian, correlation, covariance) and of the agglomeration rule

Write the solution of each exercise in a separate R script file. 
After this, write a report in R markdown that will incoroporate the R files, and comment the results. 



